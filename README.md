# ğŸ› ï¸ ETL em Python para Arquivo CSV Grande com Carga no SQL Server

Este projeto realiza um processo **ETL (Extract, Transform, Load)** eficiente para arquivos `.csv` de grande porte, realizando:

- Leitura por chunks (blocos)
- Tratamento de erros e dados invÃ¡lidos
- Carga incremental em banco de dados **SQL Server**
- Registro de linhas com falhas em um arquivo separado

---

## ğŸ¯ Objetivo do Projeto

O objetivo deste processo de ETL foi **disponibilizar um grande volume de dados estruturados em um banco SQL Server** para que a equipe de **Cientistas de Dados** pudesse aplicar regras de **InteligÃªncia de NegÃ³cio (BI)** e realizar anÃ¡lises avanÃ§adas.

Durante o desenvolvimento, aprendi e apliquei boas prÃ¡ticas relacionadas a:

- ManipulaÃ§Ã£o de dados extensos em Python
- Tratamento e limpeza de dados em larga escala
- ConexÃ£o segura e eficiente com bancos de dados
- OtimizaÃ§Ã£o de leitura de arquivos CSV grandes com `chunksize`
- Registro e rastreamento de erros para garantir integridade dos dados

---

## ğŸš€ Tecnologias Utilizadas

- Python 3.x
- Pandas
- PyODBC
- SQL Server
- CSV (como fonte de dados)

---
